import numpy  #numpy is used to make some operrations with arrays more easily
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

__errors__= [];  #global variable to store the errors/loss for visualisation

def h(params, sample):
	"""This evaluates a generic linear function h(x) with current parameters.  h stands for hypothesis

	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		sample (lst) a list containing the values of a sample 

	Returns:
		Evaluation of h(x)
	"""
	acum = 0
	for i in range(len(params)):
		acum = acum + params[i]*sample[i]  #evaluates h(x) = a+bx1+cx2+ ... nxn.. 
	return acum


def show_errors(params, samples,y):
	"""Appends the errors/loss that are generated by the estimated values of h and the real value y
	
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
	
	"""
	global __errors__
	error_acum =0
#	print("transposed samples") 
#	print(samples)
	for i in range(len(samples)):
		hyp = h(params,samples[i])
		print( "hyp  %f  y %f " % (hyp,  y[i]))   
		error=hyp-y[i]
		error_acum=+error**2 # this error is the original cost function, (the one used to make updates in GD is the derivated verssion of this formula)
	mean_error_param=error_acum/len(samples)
	__errors__.append(mean_error_param)

def GD(params, samples, y, alfa):
    """Gradient Descent algorithm 
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
		samples (lst) a 2 dimensional list containing the input samples 
		y (lst) a list containing the corresponding real result for each sample
		alfa(float) the learning rate
	Returns:
		temp(lst) a list with the new values for the parameters after 1 run of the sample set
	"""
    print(params, 'params')
    temp = list(params)
    print(temp, 'temp')
    general_error=0
    for j in range(len(params)):
        acum =0; error_acum=0
        for i in range(len(samples)):
            error = h(params,samples[i]) - y[i]
            print(error, 'error')
            acum = acum + error*samples[i][j]  #Sumatory part of the Gradient Descent formula for linear Regression.
        temp[j] = params[j] - alfa*(1/len(samples))*acum  #Subtraction of original parameter value with learning rate included.
    return temp

def scaling(samples):
	"""Normalizes sample values so that gradient descent can converge
	Args:
		params (lst) a list containing the corresponding parameter for each element x of the sample
	Returns:
		samples(lst) a list with the normalized version of the original samples
	"""
	acum =0
	samples = numpy.asarray(samples).T.tolist() 
	for i in range(1,len(samples)):	
		for j in range(len(samples[i])):
			acum=+ samples[i][j]
		avg = acum/(len(samples[i]))
		max_val = max(samples[i])
		#print("avg %f" % avg)
		#print(max_val)
		for j in range(len(samples[i])):
			#print(samples[i][j])
			samples[i][j] = (samples[i][j] - avg)/max_val  #Mean scaling
	return numpy.asarray(samples).T.tolist()


#  univariate example
#params = [0,0]
#samples = [1,2,3,4,5]
#y = [2,4,6,8,10]

#  multivariate example trivial
renamed = {
    'Heisman': 'Heisman',
    'Pct': 'Completion Percentage',
    'Y/A.1':  'Yards per Attempt',
    'Rate.1': 'Efficiency Rating', 
    'Rate': 'QB Rating',
    'Sk%': 'Sacked %'
}
df = pd.read_excel('../datasets/collegeToPros.xlsx', header=1, usecols=[4, 6, 20, 28, 29, 30, 31])
df.rename(columns = renamed, inplace = True)
df['Sacked %'] = df['Sacked %']*100
df_y = df['QB Rating']
df_x = df[['Draft',
    'Sacked %',
    'Completion Percentage',
    'Yards per Attempt',
    'Efficiency Rating']]
X = df_x
y = df_y

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

#  multivariate example
params = [0,0,0,0,0]
#samples = [[1,1],[2,2],[3,3],[4,4],[5,5],[2,2],[3,3],[4,4]]
#y = [2,4,6,8,10,2,5.5,16]

alfa = 0.001  #  learning rate
samples = numpy.asarray(X_train)
y_test_array = numpy.asarray(y_test)
y_hand = numpy.asarray(y_train)
print(y_hand, 'y hand')
x_test = numpy.asarray(X_test)

for i in range(len(samples)):
	if isinstance(samples[i], list):
		samples[i]=  samples[i]
	else:
		samples[i]=  samples[i]
print ("original samples:")
#print (samples)
#samples = scaling(samples)
print ("scaled samples:")
#print (samples)


epochs = 0

while True:  #  run gradient descent until local minima is reached
	oldparams = list(params)
	print (params)
	params=GD(params, samples,y_hand,alfa)	
	show_errors(params, samples, y_hand)  #only used to show errors, it is not used in calculation
	print (params)
	epochs = epochs + 1
	if(oldparams == params or epochs == 1):   #  local minima is found when there is no further improvement
		print ("samples:")
		print(samples)
		print ("final params:")
		print (params)
		break

import matplotlib.pyplot as plt  #use this to generate a graph of the errors/loss so we can see whats going on (diagnostics)
plt.plot(__errors__)
plt.show(block=True)

# Make predictions using the testing set
y_pred = []
print(x_test)
for x in x_test:
    aux = 0
    for i in range(5):
        aux = aux + x[i]*params[i]
    y_pred.append(aux)

# The coefficients
print('Coefficients: \n', params)
# The mean squared error
print('Mean squared error: %.2f'
    % mean_squared_error(y_test_array, y_pred))
# The coefficient of determination: 1 is perfect prediction
print('Coefficient of determination: %.2f'
    % r2_score(y_test_array, y_pred))

